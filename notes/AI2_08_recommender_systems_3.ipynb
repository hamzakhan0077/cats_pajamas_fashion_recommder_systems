{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Recommender Systems III</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br />\n",
    "    School of Computer Science and Information Technology<br />\n",
    "    University College Cork\n",
    "</h2>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Matrix Factorization</h1>\n",
    "<ul>\n",
    "    <li>We continue to look at collaborative filtering.</li>\n",
    "    <li>In the previous lecture, we saw an instance-based approach to collaborative filtering:\n",
    "        user-based nearest-neighbours.\n",
    "    </li>\n",
    "    <li>In this lecture, we will look at a model-based approach to collaborative filtering: matrix factorization.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Embeddings</h2>\n",
    "<ul>\n",
    "    <li>Consider a ratings matrix, $\\v{R}$:\n",
    "        <table style=\"border: 1px solid; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\"></th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_1$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_2$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_3$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_4$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_5$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_6$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_1$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">1</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_2$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_3$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_4$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_5$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li>In $\\v{R}$,\n",
    "        <ul>\n",
    "            <li>each user is represented by a row vector of ratings with dimension $|I|$; and</li>\n",
    "            <li>each item is represented by a (column) vector of ratings with dimension $|U|$.</li>\n",
    "        </ul>\n",
    "        These vectors have a high dimension and are sparse.\n",
    "    </li>\n",
    "    <li>So why not come up with <b>embeddings</b>:\n",
    "        <ul>\n",
    "            <li>These would map the high dimensional sparse vectors to low-dimensional dense vectors. \n",
    "                (This should sound familiar!)\n",
    "            </li>\n",
    "        </ul>\n",
    "        But we will map users and items to the same space.\n",
    "        <ul>\n",
    "            <li>In other words, they will map to vectors that have the same dimension, call it $d$.</li>\n",
    "            <li>Each element represents a feature.</li>\n",
    "            <li>In the case of user embeddings, the values indicate how much the user likes that feature.</li>\n",
    "            <li>In the case of item embeddings, the values indicate how much the item possesses that feature.</li>\n",
    "        </ul>\n",
    "        So, let the embedding for user $u$ be a row vector of dimension $d$ and refer to it as $\\v{P}^{(u)}$.\n",
    "        And let the embedding for item $i$ be a (column) vector of dimension $d$ and refer to it as $\\v{Q}^{(i)}$.\n",
    "    </li>\n",
    "    <li>Predicting $\\hat{r}_{ui}$.\n",
    "        <ul>\n",
    "            <li>We can predict $\\hat{r}_{ui}$ by computing the product of the user embedding and item embedding\n",
    "                $$\\hat{r}_{ui} = \\v{P}^{(u)}\\v{Q}^{(i)}$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Example for $d=3$:\n",
    "        <ul>\n",
    "            <li>Consider $u_2$.\n",
    "                <ul>\n",
    "                    <li>Her ratings are $\\rv{5,5,\\bot,3,4,\\bot}$.</li>\n",
    "                    <li>Suppose the corresponding embedding is $\\rv{2.8,1.4,-1.75}$ (never mind where this comes\n",
    "                        from for the moment).\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>And consider $i_3$.\n",
    "                <ul>\n",
    "                    <li>Its ratings are $\\cv{5\\\\\\bot\\\\\\bot\\\\3\\\\4}$</li>\n",
    "                    <li>Suppose the corresponding embedding is $\\cv{0.0\\\\2.5\\\\0.3}$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Then the predicted rating of $u_2$ for $i_3$ is the product of the two embeddings, i.e.\n",
    "                $$2.8\\times0.0 + 1.4\\times2.5 + -1.75\\times0.3 = 3\\,\\, \\mathit(approx)$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Latent features</h2>\n",
    "<ul>\n",
    "    <li>The explanation should so far be very reminiscent of the simple content-based recommender from two lectures ago.</li>\n",
    "    <li>Things that feel the same:\n",
    "        <ul>\n",
    "            <li>Users and items are represented by vectors of dimension $d$ in the same space.</li>\n",
    "            <li>In the case of users, we can think of the values as being how much a user likes a feature.</li>\n",
    "            <li>In the case of items, we can think of the values as being how much an item possesses that feature.</li>\n",
    "            <li>Predictions involve computing the product of two vectors.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Things that feel different:\n",
    "        <ul>\n",
    "            <li>Previously, we <em>designed</em> the features, e.g. movie genres.\n",
    "                But here, the features and the values are <em>learned</em> from the ratings data. We refer to these\n",
    "                features as <b>latent features</b>, reflecting the idea that they are somehow hidden (latent) in\n",
    "                the ratings data and that we are revealing them through a learning algorithm.\n",
    "            </li>\n",
    "            <li>Previously, the product was to be thought of as measuring the similarity of the user and item.\n",
    "                Here, it is a predicted rating, and this is how we learn the features.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning the embeddings</h2>\n",
    "<ul>\n",
    "    <li>We need to learn all the user embedings $\\v{P}$ and all the item embeddings $\\v{Q}$. How?</li>\n",
    "    <li>We need a loss function. We can use, e.g., MSE.\n",
    "        $$J(\\v{P}, \\v{Q}) = \\frac{1}{|\\Omega|}\\sum_{\\langle u,i,r\\rangle \\in \\Omega} (\\v{P}^{(u)}\\v{Q}^{(i)} - r)^2$$\n",
    "        (Recall that $\\Omega$ is the set of ratings in $\\v{R}$ that are not $\\bot$.)\n",
    "    </li>\n",
    "    <li>(By the way, the loss function in this case is not convex.)\n",
    "    </li>\n",
    "    <li>Then, we can use, for example, Gradient Descent.</li>\n",
    "    <li>This, for example, is Stochastic Gradient Descent:\n",
    "        <ul style=\"background: lightgrey;\">\n",
    "            <li>initialise $\\v{P}$ and $\\v{Q}$ randomly</li>\n",
    "            <li>repeat until convergence\n",
    "                <ul>\n",
    "                    <li>repeat $|\\Omega|$ times\n",
    "                        <ul>\n",
    "                            <li>select $\\langle u,i,r\\rangle$ from $\\Omega$ at random</li>\n",
    "                            <li>$\\v{P}^{(u)} \\gets \\v{P}^{(u)} - \\alpha \\times (\\v{P}^{(u)}\\v{Q}^{(i)} - r) \\times \\v{Q}^{(i)}$</li>\n",
    "                            <li>$\\v{Q}^{(i)} \\gets \\v{Q}^{(i)} - \\alpha \\times (\\v{P}^{(u)}\\v{Q}^{(i)} - r) \\times \\v{P}^{(u)}$</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Matrix completion</h2>\n",
    "<ul>\n",
    "    <li>In the presentation of this material above, we concentrated on individual predictions, $\\v{P}^{(u)}\\v{Q}^{(i)}$.</li>\n",
    "    <li>But, if we multiply matrices $\\v{P}$ and $\\v{Q}$, $\\v{P}\\v{Q}$, then we get all predictions at once!</li>\n",
    "    <li>This is why some people refer to this as <b>matrix completion</b>: we're getting predictions for all the\n",
    "        entries that are $\\bot$ (and all those that are not $\\bot$).\n",
    "    </li>\n",
    "    <li>It is also why this is <b>matrix factorization</b> (i.e. factorization is writing one thing as a product of other things).\n",
    "        What we doing is finding $\\v{P}$ and $\\v{Q}$ such that\n",
    "        $$\\v{P}\\v{Q} \\approx \\v{R}$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Discussion of matrix factorization</h2>\n",
    "<ul>\n",
    "    <li>Advantages of matrix factorization for collaborative filtering include:\n",
    "        <ul>\n",
    "            <li>It does not require any item or user descriptions, just user-item interactions (e.g. ratings) &mdash;\n",
    "                and this is data we will collect during the normal operation of the system.\n",
    "            </li>\n",
    "            <li>It may recommend items that are pleasantly surprising (certainly more so than content-based\n",
    "                approaches), since it recommends using <em>other peoples'</em> tastes.\n",
    "            </li>\n",
    "            <li>It is fast at prediction time.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Its disadvantages include:\n",
    "        <ul>\n",
    "            <li>Learning the embeddings (the SGD above) takes time. So new ratings generally cannot take \n",
    "                immediate effect. They will have to be buffered until the next time the model gets updated\n",
    "                (e.g. every night). (There are, however, some incremental versions of matrix factorization,\n",
    "                which do allow new ratings to take immediate effect.)\n",
    "            </li>\n",
    "            <li>It has problems recommending to cold-start users and recommending cold-start items.</li>\n",
    "            <li>It can exhibit popularity bias: over-recommending popular items (although this may depend to\n",
    "                some extent on details of the implementation).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Can it explain its recommendations?\n",
    "        <ul>\n",
    "            <li>The basic answer is, No. The latent features do not mean anything to human users. (Of course, there\n",
    "                is research that tries to constrain the latent features in various ways to try to make them more\n",
    "                human-interpretable.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In concluding, let's mention some variants.\n",
    "        <ul>\n",
    "            <li>It is normal to use slightly more complicated formulae to learn values referred to as\n",
    "                user biases and item biases&mdash; these help deal with the problems with ratings scales that we\n",
    "                mentioned in the previous lecture.\n",
    "            </li>\n",
    "            <li>It is normal to use regularization.</li>\n",
    "            <li>There are variants that combine with nearest-neighbours in various ways.\n",
    "                <!-- E.g. learn embeddings then run user-based knn on these instead of the ratings vectors.\n",
    "                     E.g. learn the similarities of a usr-based knn model in a way similar to MF.\n",
    "                     E.g. combine the previous one with MF.\n",
    "                  -->\n",
    "            </li>\n",
    "            <li>There are variants that allow item descriptions, user descriptions and contextual information\n",
    "                to be added to $\\v{R}$ in various ways, thus giving a system that handles all the kinds of\n",
    "                data that we may have.\n",
    "                <!-- E.g. block 00 is R, block 01 adds user descriptions as extra columns, block 10 adds item\n",
    "                     descriptions as extra rows, block 11 is not used, then factorize.\n",
    "                     E.g. factorization machines.\n",
    "                     And so on.\n",
    "                 -->\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Top-N Recommendation</h1>\n",
    "<ul>\n",
    "    <li>Recall that recommender systems typically proceed through (at least) three steps:\n",
    "        <figure>\n",
    "            <img src=\"images/rs_arch.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>We've now looked at several algorithms for scoring.\n",
    "    </li>\n",
    "    <li>Let's repeat what we said previously about the third step in this diagram:\n",
    "        <ul>\n",
    "            <li>The obvious thing to do is to select the $N$ candidates whose scores are highest and recomend these to the user.</li>\n",
    "        </ul>\n",
    "        However, there may be some additional criteria to take into account at this stage. Some examples include:\n",
    "        <ul>\n",
    "            <li>There may be some business rules to take into account. For example, there may be some items the business\n",
    "                is trying to push. So there may be a rule that requires that one or more slots in the top-$N$ are occupied by these items, displacing the 'organic' recommendations. (Think about sponsored content, for example.)\n",
    "            </li>\n",
    "            <li>We might have more than one recommender model whose scores we want to combine.</li>\n",
    "            <li>We often carry out some re-ranking at this stage in order to ensure that the top-$N$ has a \n",
    "                degree of diversity or some notion of fairness.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let's look at just one example: diversity.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Top-$N$ Diversity</h2>\n",
    "<ul>\n",
    "    <li>Suppose the user likes fantasy and thrillers and comedies. And suppose we recommend the $N$ candidate \n",
    "        items that obtained the highest scores (highest predicted rating\n",
    "        in our case).\n",
    "        <ul>\n",
    "            <li>Maybe this is what we end up recommending:\n",
    "                <figure>\n",
    "                    <img src=\"images/top-N.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Each recommendation is <em>relevant</em> to the user. She likes fantasy! But this top-$N$ lacks\n",
    "        <b>diversity</b>.\n",
    "        <ul>\n",
    "            <li>A more diverse top-$N$ (e.g. containing at least one thriller, at least one comedy)\n",
    "                would be more likely to include at least one recommendation that would\n",
    "                satisfy the user.\n",
    "            </li>\n",
    "            <li>It would give her a meaningful choice.</li>\n",
    "            <li>It would be one way of handling the recommender's uncertainty about the user's preferences.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining diversity</h2>\n",
    "<ul>\n",
    "    <li>Diversity is a property of a set of items, not a property of an individual item.</li>\n",
    "    <li>Suppose we have a set of recommendations $S$. We can measure the <em>marginal</em>\n",
    "        increase in diversity obtained by adding item $i$ into set $S$ as the maximum\n",
    "        distance (smallest similarity) between $i$ and the members of $S$:\n",
    "        $$div(i, S) = max_{j \\in S}(1 - sim(i, j))$$\n",
    "    </li>\n",
    "    <li>E.g. if we add another Star Wars movie to the top-$N$ shown previously, it is very similar\n",
    "        to the ones we have already, so its marginal diversity is very low.\n",
    "    </li>\n",
    "    <li>But if we add a comedy to the top-$N$, then it is not very similar to the movies we have\n",
    "        already, so its marginal diversity is very high.\n",
    "    </li>\n",
    "    <li>Similarity can be measured in any of the ways we have discussed already, e.g. cosine or\n",
    "        Pearson of vectors whose features are genres or ratings or latent features.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Greedy re-ranking</h2>\n",
    "<ul>\n",
    "    <li>So, instead of recommending the $N$ with the highest predicted ratings, we select the\n",
    "        $N$ that achieve the best balance between relevance and diversity, controlled by a hyperparameter\n",
    "        $\\lambda \\in [0, 1]$:\n",
    "        <ul style=\"background: lightgrey;\">\n",
    "            <li>$S \\gets [\\,\\,]$</li>\n",
    "            <li>while $|S| < N$\n",
    "                <ul>\n",
    "                    <li>$i^* \\gets \\arg\\max_{i \\in candidates} \\hat{r}_{ui} + \\lambda div(i,S)$</li>\n",
    "                    <li>delete $i^*$ from candidates</li>\n",
    "                    <li>append $i^*$ to the end of $S$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>return $S$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>As usual, there are lots of variations on this, especially lots of different ways of defining\n",
    "        diversity.\n",
    "    </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
