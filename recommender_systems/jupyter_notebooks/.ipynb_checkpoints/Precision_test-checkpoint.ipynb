{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb453d82",
   "metadata": {},
   "source": [
    "<h1> Precision Test </h1>\n",
    "<ul>\n",
    "    <li>Random Recommender</li>\n",
    "    <li>Popularity Based Recommender</li>\n",
    "    <li>Test Set Creation</li>\n",
    "    <li>Popularity Based Recommender Precision</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669bfe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf630c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf064f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26722, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv(\"../datasets/articles_transactions_5.csv\")\n",
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b874c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"../datasets/articles_transactions_5.csv\")\n",
    "T = pd.read_csv(\"../datasets/transactions_5.csv\")\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "i = articles_df.copy()\n",
    "i['detail_desc'] = i['detail_desc'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815eff0",
   "metadata": {},
   "source": [
    "<h2>Random Recommender</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0124ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_recommender(customer):\n",
    "    # print(f\"{customer.customer_id} \", np.random.choice(articles_df['article_id'].values,5))\n",
    "    return random.sample(sorted(articles_df['article_id'].values),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489653ef",
   "metadata": {},
   "source": [
    "<h2>Popularity Based Recommender</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e29d2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_recommender(customer):\n",
    "    # Top 5 most bought\n",
    "    popular_products = T['article_id'].value_counts().nlargest(5).to_frame()['article_id'].keys().values\n",
    "    return popular_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1189969c",
   "metadata": {},
   "source": [
    "<h2>Content Based Recommender Using TFID\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6f13f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26722, 2148)\n",
      "[[1.         0.15415515 0.15415515 ... 0.         0.17445125 0.06544307]\n",
      " [0.15415515 1.         1.         ... 0.03417172 0.02818911 0.18738553]\n",
      " [0.15415515 1.         1.         ... 0.03417172 0.02818911 0.18738553]\n",
      " ...\n",
      " [0.         0.03417172 0.03417172 ... 1.         0.         0.01974166]\n",
      " [0.17445125 0.02818911 0.02818911 ... 0.         1.         0.05267121]\n",
      " [0.06544307 0.18738553 0.18738553 ... 0.01974166 0.05267121 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "candidate_profile_X = vectorizer.fit_transform(i[\"detail_desc\"])\n",
    "print(candidate_profile_X.shape)\n",
    "cosine_similarity = linear_kernel(candidate_profile_X,candidate_profile_X) \n",
    "print(cosine_similarity)\n",
    "indices = pd.Series(i.index,index=i[\"article_id\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95dc0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_Tfid(article_id,cosine_similarity):\n",
    "    idx = indices[article_id]\n",
    "    scores = enumerate(cosine_similarity[idx])\n",
    "    scores = sorted(scores,key=lambda val:val[1])\n",
    "    scores = scores[-6:-1]\n",
    "    # scores = scores[-2:-1] # we want the one before the Identical one (THE MOST SIMILAR)\n",
    "    # print(scores)\n",
    "    # return [i['article_id'].iloc[tar[0]] for tar  in scores]\n",
    "    return scores # now returns a list of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01cd8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_recommender_Tfid(customer):\n",
    "    tuple_list = [] # list with similarity and index\n",
    "    customer_purchases = T['article_id'][T['customer_id'] == customer].drop_duplicates().values\n",
    "    for product in customer_purchases:\n",
    "        tuple_list += rec_Tfid(product,cosine_similarity)\n",
    "    tuple_list = set(tuple_list)\n",
    "    tuple_list = list(tuple_list)\n",
    "    scores = sorted(tuple_list, key=lambda val: val[1])\n",
    "    scores = scores[-6:-1] # pick bottom 5, with the highest scores\n",
    "    return [i['article_id'].iloc[tar[0]] for tar in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d3c8a",
   "metadata": {},
   "source": [
    "<h2>Content Based Recommender Using Count Vectorizer\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(binary=True,stop_words = \"english\")\n",
    "profiles = count_vectorizer.fit_transform(i[\"detail_desc\"])\n",
    "similarity = linear_kernel(profiles,profiles) \n",
    "indices = pd.Series(i.index,index=i[\"article_id\"]).drop_duplicates()\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_count(article_id,similarity):\n",
    "    idx = indices[article_id]\n",
    "#     print(idx)\n",
    "    scores = enumerate(similarity[idx])\n",
    "    scores = sorted(scores,key=lambda val:val[1])\n",
    "    scores = scores[-6:-1]\n",
    "    # scores = scores[-2:-1] # we want the one before the Identical one (THE MOST SIMILAR)\n",
    "    # print(scores)\n",
    "    # return [i['article_id'].iloc[tar[0]] for tar  in scores]\n",
    "    return scores # now returns a list of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_recommender_count(customer):\n",
    "    tuple_list = [] # list with similarity and index\n",
    "    customer_purchases = T['article_id'][T['customer_id'] == customer].drop_duplicates().values\n",
    "    for product in customer_purchases:\n",
    "        tuple_list += rec_count(product,similarity)\n",
    "    tuple_list = set(tuple_list)\n",
    "    tuple_list = list(tuple_list)\n",
    "    scores = sorted(tuple_list, key=lambda val: val[1])\n",
    "    scores = scores[-6:-1] # pick bottom 5, with the highest scores\n",
    "    return [i['article_id'].iloc[tar[0]] for tar in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eba831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(content_based_recommender_count('65cb62c794232651e2ac711faa11c2b4e3d41d5f3b59b50bee3ffde1d5776644'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccaae31",
   "metadata": {},
   "source": [
    "<h2>Test Set Creation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b263453",
   "metadata": {},
   "source": [
    "<p>\n",
    "From the Kaggle transactions, extract a set T for just one month.\n",
    "Let U be the users who have at least one transaction inT.\n",
    "Let I be the items that are in at least one transaction in T.\n",
    "\n",
    "For any user u who has fewer than 5 transactions in T, \n",
    "- delete u from U\n",
    "- delete u's transactions from T.\n",
    "\n",
    "From U, choose 1000 users at random. Call these test-U.\n",
    "\n",
    "For each user u in test-U, \n",
    "- move the last 20% of their transactions from T to test-T (i.e. delete them from T, insert them into test-T).\n",
    "\n",
    "Train a recommender system on T. (This does not apply to the random recommender, but it does apply to the popularity recommender because you need to know which items in T are the most popular.)\n",
    "\n",
    "Test the recommender as follows; for each u in test-U,\n",
    "- obtain n recommendations (e.g. 5 recommendations)\n",
    "- compute precision (based on how many of u's recommendations are in test-T for user u)\n",
    "Afterwards, you have the precision for each user in test-U. So now compute the mean of these.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = T['customer_id'].drop_duplicates().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1658b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = T['article_id'].drop_duplicates().to_frame()\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab24620",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.shape\n",
    "test_u = u.sample(n=1000, random_state = 1)\n",
    "test_t = pd.DataFrame()\n",
    "for k,cust in enumerate(test_u['customer_id']):  \n",
    "    cust_transac = T[T['customer_id'] == cust]\n",
    "    bottom_transac = cust_transac[-1 * round(0.20 * len(cust_transac)):]\n",
    "    test_t = test_t.append(bottom_transac)\n",
    "    indexs = bottom_transac.index\n",
    "    T.drop(labels = indexs, axis = 0,inplace=True )\n",
    "    print(k)\n",
    "test_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23988e",
   "metadata": {},
   "source": [
    "<h2>Polularity and Random Recommender Precision</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16221b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list_popularity = []\n",
    "precision_list_random = []\n",
    "precision_list_content = []\n",
    "precision_list_content_count = []\n",
    "for j,user in tqdm(enumerate(test_u['customer_id'])):\n",
    "    popular_recommendations = popularity_recommender(user)\n",
    "    random_recommendations = random_recommender(user)\n",
    "    content_recommendations = content_based_recommender_Tfid(user)\n",
    "    content_recommendations_count = content_based_recommender_count(user)\n",
    "    \n",
    "    u_purchases = test_t[test_t['customer_id'] == user]['article_id'].values\n",
    "    \n",
    "    precision_list_popularity.append(len(np.intersect1d(popular_recommendations,u_purchases)))\n",
    "    precision_list_random.append(len(np.intersect1d(random_recommendations,u_purchases)))\n",
    "    precision_list_content.append(len(np.intersect1d(content_recommendations, u_purchases)))\n",
    "    precision_list_content_count.append(len(np.intersect1d(content_recommendations_count, u_purchases)))\n",
    "    \n",
    "#     print(j)\n",
    "print(\"MEAN Precison of Popularity Recommender\",sum(precision_list_popularity)/len(precision_list_popularity))\n",
    "print(\"MEAN Precison of Random Recommender\",sum(precision_list_random)/len(precision_list_random))\n",
    "print(\"MEAN Precison of Content Based Recommender TFID\" ,sum(precision_list_content)/len(precision_list_content))\n",
    "print(\"MEAN Precison of Content Based Recommender COUNT\" ,sum(precision_list_content_count)/len(precision_list_content_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29edcc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
